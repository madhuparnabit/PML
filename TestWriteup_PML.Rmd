# Practical Machine Learning Prediction Assignment

##Libraries
I used the following packages not listed in the course materials.


``` {r part0}

library(caret)
library(rpart)
library(rpart.plot)
library(RColorBrewer)
library(rattle)
library(randomForest)
library(knitr)
library(gbm)
library(e1071)
library(plyr)
```
## Loading Training Data & Cleaning it
The pml-training.csv data is actually used to devise training and testing sets.
We first download the files and upload the files into R, interpreting the miscellaneous NA, #DIV/0! and empty fields as NA
```{r part1}
setwd("D:/coursera/PML/")
getwd()

training.ds<-read.csv("pml-training.csv", na.strings=c("NA","#DIV/0!", ""))
testing.ds <-read.csv("pml-testing.csv", na.strings=c("NA", "#DIV/0!", ""))
training.ds<-training.ds[,colSums(is.na(training.ds)) == 0]
testing.ds<-testing.ds[,colSums(is.na(testing.ds)) == 0]
```

## Cleaning Data
The features user_name raw_timestamp_part_1 raw_timestamp_part_2 cvtd_timestamp new_window num_window are not related to calculations and are removed form the downloaded data
```{r part2}
training.ds<-training.ds[,-c(1:7)]
testing.ds<-testing.ds[,-c(1:7)]
```

## partition training dataset into 60/40 train/test##


```{r part3}
library(caret)
train_set<- createDataPartition(training.ds$classe, p = 0.6, list = FALSE)
training <-training.ds[train_set, ]
testing <-training.ds[-train_set, ]
```


##Data Manipulation & Variable Selection
We look at the relative importance of the covariates using the output of a Random Forest algorithm (which we call directly using randomForest() rather than the caret package purely for speed purposes as we cannot specify the number of trees to use in caret), and plotting variable importance using varImpPlot(). A model with top 10 important parameters is certainly preferred if the accuracy of model with fewer covariates is same as model with more covariates but when we choose top 10 important variables the model accuracy was much lower than a model with 53 parameters so I've chosen to use all the 53 variables.
```{r part4}
library(randomForest)
set.seed(6142662)
fitModel <- randomForest(classe~., data=training, importance=TRUE, ntree=100)
varImpPlot(fitModel)
```


## MODELING: 
Applying decision trees algorithm For Prediction and Confusion Matrix for Testing. 
```{r part5}
 set.seed(12345)
library(rpart)
library(rpart.plot)
modfit1 <- rpart(classe ~ ., data=training, method="class")
fancyRpartPlot(modfit1)
predict1 <- predict(modfit1,testing, type = "class")

cm <- confusionMatrix(predict1, testing$classe)
cm
```
##Prediction with Random Forests and Confusion Matrix for Testing##
```{r part6}
set.seed(12345)
modfit2 <- randomForest(classe ~ ., data=training)
predict2 <- predict(modfit2, testing, type = "class")
cm2 <- confusionMatrix(predict2, testing$classe)
cm2
```
##Prediction with Generalized Boosted Regression and Confusion Matrix for Testing##

```{r part7}
set.seed(12345)
fit <- trainControl(method = "repeatedcv",
                    number = 5,
                    repeats = 1)

gbmFit1 <- train(classe ~ ., data=training, method = "gbm",
                 trControl = fit,
                 verbose = FALSE)


gbmMod <- gbmFit1$finalModel

gbmPred <- predict(gbmFit1, newdata=testing)
gbmAccuracy <- confusionMatrix(gbmPred, testing$classe)
gbmAccuracy

plot(gbmFit1, ylim=c(0.9, 1))
```

##Predicting Results on the Test Data - Conclusion

Random Forests gave an Accuracy in the testing dataset of 99.32%, which was more accurate that what I got from the Decision Trees or GBM. The expected out-of-sample error is 0.68% (i.e. 100-99.32),as the results were satisfactory,I did not go ahead with cross validation,as I have tried 3 different algorithms for the same,the improvement expectation was not much at this stage through cross validation.
```{r part8}
prediction2 <- predict(modfit2, testing, type = "class")

```

##Coursera provided code for submission

Set these `R` values from previous code that cleans data to match submission code.
```{r}
feature_set <- colnames(training.ds)
newdata     <- testing.ds
```

Method to write answers to separate `.txt` files
```
pml_write_files = function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("problem_id_",i,".txt")
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}
```

Predict the `answers` to the 20 questions.
```
x <- evaluation_data
x <- x[feature_set[feature_set!='classe']]
answers <- predict(rf, newdata=x)
```
Now check
```
answers
```
Now write files and go to submission at coursera
```
pml_write_files(answers)
```


